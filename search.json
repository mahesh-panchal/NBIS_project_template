[
  {
    "objectID": "working_habits.html",
    "href": "working_habits.html",
    "title": "How to use this template",
    "section": "",
    "text": "Use an organised folder structure.\nMake a private Project repository on Github, and clone it on Uppmax and then locally.\nHave a stable main git branch.\nGit branches are used to develop new features and add exploratory analyses.\nMake a test data set for development purposes.\nUse toy examples for exploring Nextflow functionality.\nWrite processes in a modular way to use existing containers.\nUse Docker to make containers for tools which are not available as existing container images.\nUse Nextflow to manage intermediate files.\nIf a script is failing, debug it in the Nextflow work directory.\nparameters and config are included in version control.\n\n\n\nThe structure I’ve found that works well for me is:\n/proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;/       (NAISS Compute Allocation)\n |\n | - README.md                                 Project details summary\n |\n | - analyses/                                 Analysis configuration files\n |   | - YYYY-MM-DD_workflow_dev                 Configuration to use test data\n |   \\ - YYYY-MM-DD_full_data_analysis           Configuration to use all the data\n | - conda/nextflow-env                        Conda Environment containing tools and dependancies to run Nextflow\n | - docs/                                     Project documentation\n \\ - workflow/                                 Nextflow workflow\n     | - bin                                     Custom script folder\n     | - configs                                 General workflow configuration\n     \\ - containers                              Custom container definitions\n\n/proj/naiss20xx-yy-zz/                         (NAISS Storage Allocation)\n |\n | - nobackup/nxf-work                         Intermediate analysis files\n \\ - NBIS_support_&lt;id&gt;_results/                Analysis results\nThis is flexible enough for both data analysis and pipeline development projects. For public pipeline development projects the public GitHub repo is used, instead of a repository workflow folder.\nThe files and folders README.md, analyses, docs, and workflow are also tracked using Git."
  },
  {
    "objectID": "working_habits.html#summary-of-work-habits",
    "href": "working_habits.html#summary-of-work-habits",
    "title": "How to use this template",
    "section": "",
    "text": "Use an organised folder structure.\nMake a private Project repository on Github, and clone it on Uppmax and then locally.\nHave a stable main git branch.\nGit branches are used to develop new features and add exploratory analyses.\nMake a test data set for development purposes.\nUse toy examples for exploring Nextflow functionality.\nWrite processes in a modular way to use existing containers.\nUse Docker to make containers for tools which are not available as existing container images.\nUse Nextflow to manage intermediate files.\nIf a script is failing, debug it in the Nextflow work directory.\nparameters and config are included in version control.\n\n\n\nThe structure I’ve found that works well for me is:\n/proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;/       (NAISS Compute Allocation)\n |\n | - README.md                                 Project details summary\n |\n | - analyses/                                 Analysis configuration files\n |   | - YYYY-MM-DD_workflow_dev                 Configuration to use test data\n |   \\ - YYYY-MM-DD_full_data_analysis           Configuration to use all the data\n | - conda/nextflow-env                        Conda Environment containing tools and dependancies to run Nextflow\n | - docs/                                     Project documentation\n \\ - workflow/                                 Nextflow workflow\n     | - bin                                     Custom script folder\n     | - configs                                 General workflow configuration\n     \\ - containers                              Custom container definitions\n\n/proj/naiss20xx-yy-zz/                         (NAISS Storage Allocation)\n |\n | - nobackup/nxf-work                         Intermediate analysis files\n \\ - NBIS_support_&lt;id&gt;_results/                Analysis results\nThis is flexible enough for both data analysis and pipeline development projects. For public pipeline development projects the public GitHub repo is used, instead of a repository workflow folder.\nThe files and folders README.md, analyses, docs, and workflow are also tracked using Git."
  },
  {
    "objectID": "test_dataset.html",
    "href": "test_dataset.html",
    "title": "How to use this template",
    "section": "",
    "text": "Using a test dataset can help rapidly develop your analysis workflow. A good test data set should be small, but have enough data to get a decent portion of the way into the analysis. This makes prototyping your processes faster when you come to test if it runs. Mistakes during coding are inevitable, and the faster the error is returned, the better.\nEnabling reentrancy in Nextflow (-resume) will mean you can also let the workflow run, and it continues from the last successfully completed processes.\nThe test dataset is typically stored in nobackup on the NAISS storage allocation.\nA script is used to make sure I know how to recreate it, and what was used as input.\nExample commands:\n\nSubsample Paired-end Illumina data:\nFRACTION=0.1\nSEED=100\nseqtk sample -s\"$SEED\" \"$READ1\" \"$FRACTION\" | gzip -c &gt; \"${READ1/_R1./_R1.subsampled.}\" &\nseqtk sample -s\"$SEED\" \"$READ2\" \"$FRACTION\" | gzip -c &gt; \"${READ1/_R2./_R2.subsampled.}\"\nwait\nSubsample a bam file:\nFRACTION=0.10\nsamtools view -b -@ \"${CPUS:-10}\" -s \"$FRACTION\" -o \"${PREFIX}.subsampled_${FRACTION}.subreads.bam\" \"${PREFIX}.subreads.bam\"\nSubsample a CSV file:\nNUM_RECORDS=1000\nshuf -n \"$NUM_RECORDS\" \"${PREFIX}.csv\" &gt; \"${PREFIX}.subsampled_${NUM_RECORDS}.csv\""
  },
  {
    "objectID": "test_dataset.html#make-a-test-dataset",
    "href": "test_dataset.html#make-a-test-dataset",
    "title": "How to use this template",
    "section": "",
    "text": "Using a test dataset can help rapidly develop your analysis workflow. A good test data set should be small, but have enough data to get a decent portion of the way into the analysis. This makes prototyping your processes faster when you come to test if it runs. Mistakes during coding are inevitable, and the faster the error is returned, the better.\nEnabling reentrancy in Nextflow (-resume) will mean you can also let the workflow run, and it continues from the last successfully completed processes.\nThe test dataset is typically stored in nobackup on the NAISS storage allocation.\nA script is used to make sure I know how to recreate it, and what was used as input.\nExample commands:\n\nSubsample Paired-end Illumina data:\nFRACTION=0.1\nSEED=100\nseqtk sample -s\"$SEED\" \"$READ1\" \"$FRACTION\" | gzip -c &gt; \"${READ1/_R1./_R1.subsampled.}\" &\nseqtk sample -s\"$SEED\" \"$READ2\" \"$FRACTION\" | gzip -c &gt; \"${READ1/_R2./_R2.subsampled.}\"\nwait\nSubsample a bam file:\nFRACTION=0.10\nsamtools view -b -@ \"${CPUS:-10}\" -s \"$FRACTION\" -o \"${PREFIX}.subsampled_${FRACTION}.subreads.bam\" \"${PREFIX}.subreads.bam\"\nSubsample a CSV file:\nNUM_RECORDS=1000\nshuf -n \"$NUM_RECORDS\" \"${PREFIX}.csv\" &gt; \"${PREFIX}.subsampled_${NUM_RECORDS}.csv\""
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "How to use this template",
    "section": "",
    "text": "Tips and links to potentially useful learning resources.\n\n\n\nVersion control with Git: Software Carpentries course.\nHow to undo (almost) anything: Tips on how to undo various actions in git.\nAccidentally delete a tracked file?\ngit restore &lt;filename&gt;\nDo not add large ( &gt; 100MB ) files.\nNever commit sensitive information to a git repository e.g.,\n\nusernames and passwords\nAPI keys\n\n\n\n\n\n\nReproducible Computational Environments Using Containers: Introduction to Docker: Software Carpentries course.\nDockerfile best practices\nUse existing containers where possible:\n\nBiocontainers Registry: Containerized conda packages.\nRocker: R in Docker.\n\nRocker TidyModels: R + Rstudio + tidyverse + tidymodels + common ML packages.\n\nPython Data science: Common python dependencies for data science workflows.\n\nBe careful not to commit sensitive data into a container.\nDocker compose: Add parameters and volumes to a docker-compose.yml file to reproduce a Docker ecosystem (start multiple containers with the same settings each time).\n\n\n\n\n\nReproducible computational environments using containers: Introduction to Singularity: Software Carpentries course (Work in progress).\nBuild custom images in Docker first, for increased portabilty.\nUse existing containers where possible:\n\nBiocontainers Registry: Containerized conda packages.\nRocker: R in Docker.\n\nRocker TidyModels: R + Rstudio + tidyverse + tidymodels + common ML packages.\n\nPython Data science: Common python dependencies for data science workflows.\n\nBe careful not to commit sensitive data into a container.\n\n\n\n\n\nNextflow Training from Seqera: Training from writers of Nextflow.\nIntroduction to Bioinformatics workflows with Nextflow and nf-core: Software Carpentries Course (Work in progress).\nRNA-seq Workflow Tutorial: Tutorial writing an RNA-seq workflow to introduce Nextflow fundamentals.\nGATK Workflow Tutorial: Tutorial writing a GATK workflow to introduce Nextflow fundamentals.\nUse toy examples to test Nextflow contructs.\nChannel.of( \n    [ \"hi\",  \"there\", 1 ], \n    [ \"hi\",  \"there\", 1 ],\n    [ \"see\", \"me\",    2 ], \n    [ \"see\", \"how\",   2 ]\n  )\n  .unique()\n  .groupTuple( by:[0,2] )\n  .view()\nNextflow documentation: Nextflow Workflow writers best friend. A description of all the available Nextflow functionality.\n\n\n\n\n\nIntroduction to Conda for (Data) Scientists: Software carpentries course.\nDo not install anything into the base environment.\nI recommend mamba (install it into a conda env)."
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "How to use this template",
    "section": "",
    "text": "Tips and links to potentially useful learning resources.\n\n\n\nVersion control with Git: Software Carpentries course.\nHow to undo (almost) anything: Tips on how to undo various actions in git.\nAccidentally delete a tracked file?\ngit restore &lt;filename&gt;\nDo not add large ( &gt; 100MB ) files.\nNever commit sensitive information to a git repository e.g.,\n\nusernames and passwords\nAPI keys\n\n\n\n\n\n\nReproducible Computational Environments Using Containers: Introduction to Docker: Software Carpentries course.\nDockerfile best practices\nUse existing containers where possible:\n\nBiocontainers Registry: Containerized conda packages.\nRocker: R in Docker.\n\nRocker TidyModels: R + Rstudio + tidyverse + tidymodels + common ML packages.\n\nPython Data science: Common python dependencies for data science workflows.\n\nBe careful not to commit sensitive data into a container.\nDocker compose: Add parameters and volumes to a docker-compose.yml file to reproduce a Docker ecosystem (start multiple containers with the same settings each time).\n\n\n\n\n\nReproducible computational environments using containers: Introduction to Singularity: Software Carpentries course (Work in progress).\nBuild custom images in Docker first, for increased portabilty.\nUse existing containers where possible:\n\nBiocontainers Registry: Containerized conda packages.\nRocker: R in Docker.\n\nRocker TidyModels: R + Rstudio + tidyverse + tidymodels + common ML packages.\n\nPython Data science: Common python dependencies for data science workflows.\n\nBe careful not to commit sensitive data into a container.\n\n\n\n\n\nNextflow Training from Seqera: Training from writers of Nextflow.\nIntroduction to Bioinformatics workflows with Nextflow and nf-core: Software Carpentries Course (Work in progress).\nRNA-seq Workflow Tutorial: Tutorial writing an RNA-seq workflow to introduce Nextflow fundamentals.\nGATK Workflow Tutorial: Tutorial writing a GATK workflow to introduce Nextflow fundamentals.\nUse toy examples to test Nextflow contructs.\nChannel.of( \n    [ \"hi\",  \"there\", 1 ], \n    [ \"hi\",  \"there\", 1 ],\n    [ \"see\", \"me\",    2 ], \n    [ \"see\", \"how\",   2 ]\n  )\n  .unique()\n  .groupTuple( by:[0,2] )\n  .view()\nNextflow documentation: Nextflow Workflow writers best friend. A description of all the available Nextflow functionality.\n\n\n\n\n\nIntroduction to Conda for (Data) Scientists: Software carpentries course.\nDo not install anything into the base environment.\nI recommend mamba (install it into a conda env)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using this template",
    "section": "",
    "text": "This repository is a template for research/support projects to produce reproducible output. It describes how to work with this template and what working practices to follow."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Using this template",
    "section": "Getting started",
    "text": "Getting started\nLearning the tools used in this way of working can be a steep learning curve. However, the benefits are a well structured project, that is easy to follow, rerun, and build upon. A common issue with “Why should I learn this?” is the perceived overhead of using these tools. However, in my opinion, the ability to recover from mistakes or unexpected outcomes is greatly mitgated by using these tools, reducing overhead over time.\nThe tools used in this way of working are:\n\nGit: Version control is an important method of tracking changes and making exploratory changes that are revertable.\nNextflow: Nextflow is a workflow manager. It is ideal for processing large scale data across a wide variety of execution systems. Snakemake and WDL are examples of alternatives, but Nextflow is my favoured workflow manager.\nQuarto: Quarto is a publishing system, useful for dynamically visualising data, performing statistical analyses, and performing small scale data processing tasks. It also supports a variety of analysis languages, such as Python, R, and Julia. It also supports a wide variety of outputs including reports, presentations, wikis, and websites.\nApptainer/Docker: Apptainer and Docker are container platforms; platforms which create isolated compute environments necessary for a computation. There are many public container images available, reducing the need for building one’s own computation environment.\nConda/Mamba: Conda (or the speedier flavour Mamba) is another package management system like containers, however these environments are not isolated like container environments are, allowing tools to interact with for example your local HPC scheduler.\nMermaid Diagrams: The mermaid diagraming system is a textual way of making various diagrams that can then be displayed for example in Quarto, or on Github, and allows for a wide variety of types of diagrams.\nMarkdown: Markdown is a markup language to create formatted text. The idea being to focus on content and leave the styling to someone else (or future you). It is a language supported my many platforms. In particular it’s used to author content with Quarto, and on Github. These sites also natively render Mermaid diagrams when included into Markdown text, in addition to rendering code, and so on.\n\nThe use of these tools helps me to better communicate science and increase the reproducibility of my scientific analyses. Typically, my analyses can be reproduced with the following commands:\ncd /proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;/analyses/&lt;date&gt;_&lt;analysis&gt;/\nmamba activate /proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;/conda/nextflow-env\n./run_nextflow.sh\n\nUppmax\nThis guide is very focused towards the Swedish Research Computation infrastructure, in particular the high performance cluster (HPC) “Uppmax”. However, this way of working should be adaptable to other compute infrastructures.\nTypcially, Uppmax provides two resources; a NAISS compute allocation, and a NAISS storage allocation on Uppmax (compute allocations are sufficient for small data sets). Swedish research groups are encouraged to use the NAISS resources at Uppmax for large scale data processing. If you’re only doing computations, then you’re recommended to use a NAISS compute allocation, however, if you need to store large amounts of data (for the duration of the project), then you should also use a NAISS storage allocation too."
  },
  {
    "objectID": "index.html#my-work-environment",
    "href": "index.html#my-work-environment",
    "title": "Using this template",
    "section": "My work environment",
    "text": "My work environment\nAn overview of my general working environment.\n\nDocker installed on my personal computer.\nApptainer on Uppmax.\nA Github account.\nA Gitpod account.\nGit on my personal computer, Uppmax, and Gitpod.\nA conda environment with Nextflow and Quarto installed (local, Uppmax).\nVSCode, with syntax highlighting, linting, and formatting.\n\n\nMy toolkit\nWhen working on a project, I may work on it locally on my PC, remotely on Uppmax, or remotely on Gitpod. Using the widely supported container system Docker means I have an easier time managing and porting software installations. Although Uppmax doesn’t support Docker for various security reasons, it does support Apptainer which is able to use container images from Docker. Using Container systems means I do not have to rely on computer administrators to install tools for me. Many bioinformatic tools are also available as container images through Biocontainers, meaning you can use the container and get on with your analyses, and not waste time building images yourself.\nVersion control tools like Git are a great tool to help keep work organised, and in a way, backed up. Using git branches are a particularly powerful way to keep both a working copy of your analysis, and work on different exploratory analyses at the same time. It can also be useful for demonstrating work attribution (and accountability). Web-based git repositories such as Github, also provide a way of publishing your work (or keeping it private), can function as a backup of sorts, and provide other useful services such as automated actions, or Wiki spaces, or website hosting.\nConda (or Mamba; drop-in replacement for conda) is another software package manager, however the main difference to containerised software is that the software environment is not self-contained like containers are. You can work with tools both in your environment and outside, which is particularly useful for a tool like Nextflow. One of Nextflow’s strengths is that it can work on a wide variety of computing platforms, however in order to do so in a containerized environment would mean every tool Nextflow supports would need to be included and would need to be configured to use the correct files outside the container - a hugely complex process. It’s much simpler to provide an environment for Nextflow and allow it to interact with whatever your compute system uses. The added benefit of using a conda environment is that it can be set up for everyone part of the project, easing installation issues.\nNextflow is a workflow manager. I use it to manage the flow of scripts so I know data provenance from beginning to end. It has very good file handing and scaling properties, making script writing simpler. I can write a Nextflow process for a single set of inputs and not worry too much about coding it for multiple files. That process can also be assigned a container, providing a running environment which doesn’t have software conflicts with other things that need to be run.\nEditors with syntax highlighting and git integrations are useful coding tools. I currently use VScode, with various extensions, such as Prettier - code formatting, Nextflow syntax highlighting, and Quarto support."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website is a guide to using this template to work reproducibly."
  },
  {
    "objectID": "computation_environment.html",
    "href": "computation_environment.html",
    "title": "How to use this template",
    "section": "",
    "text": "Conda is a software package manager. It makes it easy to install tools into custom environments. Several tools can be installed into an environment, and along with it all the libraries and dependancies it needs. A conda environment is a double-edged sword in that, when activated, it is not isolated from the users normal environment. This makes it ideal for running a software like Nextflow, but perhaps troublesome for others.\nMamba is another software intended to be a drop-in replacement for Conda. A primary advantage is faster building of environments, through better package dependancy resolution.\nConda environments are activated using conda activate &lt;environment-name&gt; and deactivated using conda deactivate.\nConda is enabled on Uppmax using module load conda.\nCreate a project-wide conda environment:\nPROJECT_DIR=/proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;\nmamba env create --prefix \"$PROJECT_DIR/conda/nextflow-env\" \\\n  -f \"$PROJECT_DIR/workflow/nextflow_conda-env.yml\""
  },
  {
    "objectID": "computation_environment.html#make-the-running-environment",
    "href": "computation_environment.html#make-the-running-environment",
    "title": "How to use this template",
    "section": "",
    "text": "Conda is a software package manager. It makes it easy to install tools into custom environments. Several tools can be installed into an environment, and along with it all the libraries and dependancies it needs. A conda environment is a double-edged sword in that, when activated, it is not isolated from the users normal environment. This makes it ideal for running a software like Nextflow, but perhaps troublesome for others.\nMamba is another software intended to be a drop-in replacement for Conda. A primary advantage is faster building of environments, through better package dependancy resolution.\nConda environments are activated using conda activate &lt;environment-name&gt; and deactivated using conda deactivate.\nConda is enabled on Uppmax using module load conda.\nCreate a project-wide conda environment:\nPROJECT_DIR=/proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;\nmamba env create --prefix \"$PROJECT_DIR/conda/nextflow-env\" \\\n  -f \"$PROJECT_DIR/workflow/nextflow_conda-env.yml\""
  },
  {
    "objectID": "prototyping.html",
    "href": "prototyping.html",
    "title": "How to use this template",
    "section": "",
    "text": "To build up an analysis project, workflow development aims to follow the GitFlow model of coding.\n\nStable main git branch.\nMostly stable dev git branch - develop a line of analysis.\nFeature branch to try stuff.\n\nMake a new branch named &lt;new_feature&gt; in Git.\ngit checkout -b &lt;new_feature&gt;\nAdd a process into the workflow:\n\nNextflow processes are kept as modular as possible, often limiting them to a single tool. The benefits are that one can often use public images for a process, reducing build time. Environments are small, meaning low build time, and low conflict potential. Since projects are often incrementally developed, this reduces the need to rebuild images or environments, saving time.\nTry to use existing containers (public images) when possible. When a container must be created, I use Docker to make a local image, test, and then push it to Github packages to keep it private to the project. The README.md in the workflow/containers directory has more information on creating custom containers.\n\nprocess &lt;uppercase_name&gt; {\n\n  input:\n  path &lt;filename_var&gt;\n\n  // directives\n  &lt;directives&gt;\n  conda \"&lt;channel&gt;::&lt;software&gt;=&lt;version&gt;\"\n  container \"&lt;software&gt;:&lt;version+build&gt;\"\n\n  script:\n  \"\"\"\n  command --opts $var\n  \"\"\"\n\n  output:\n  path \"&lt;filename&gt;\", emit: &lt;file_type&gt;\n  path \"*\"  // Captures everything. Use when you don't know what the output is.\n\n}\nPass the appropriate input channel(s) in the workflow block.\n\nNextflow has a large selection of channel operators (functions that manipulate process input), which can manipulate input into any format you need. It can be difficult to get the desired input combination first time, and you do not want to experiment on your long running data sets to find out what works. To understand what a Nextflow construct does, write toy examples like in Nextflow Patterns. Nextflow also has a (GUI) console mode to test syntax (nextflow console).\n\nworkflow {\n\n  Channel.fromFilePairs( params.reads, checkIfExists: true )\n    .set{ input_ch }\n\n  FASTQC( input_ch )\n  FASTP( input_ch )\n  ASSEMBLE( FASTP.out.trimmed_reads )\n\n}\nAdd the process resources to the configs.\nprocess {\n  withName: 'FASTQC' {\n    cpus = 4\n    time = '1h'\n  }\n  withName: 'FASTP' {\n    cpus = 4\n    time = '2h'\n  }\n}\nDon’t forget to commit your changes to the feature branch as you develop:\ngit add &lt;file&gt;\ngit commit -m \"What I did\"\nMistakes to commits can be changed using:\ngit commit --amend\n\n\n\nHave a folder under analyses for development (YYYY-MM-DD_workflow_dev)\n\nparams.yml points to test data.\nUse YYYY-MM-DD to label folders and provide a natural ordering.\n\nHave another folder under analyses to analyse the real data set (YYYY-MM-DD_fulldata_analysis)\nRun stuff and break it.\n\nActivate conda environment.\nrun_nextflow.sh\nNextflow prints the working directory when a process fails.\n\n\nSee Troubleshooting for tips on debugging issues.\n\n\n\nOnce it works, merge with the existing development branch or main branch. There are two forms of merging with git. merge tacks on the changes to the end of the branch. rebase undoes your changes, applies commits to bring the branch up to date, and then reapplies your changes.\n# Update dev branch with changes from origin\ngit checkout dev\ngit pull --rebase\n\n# merge updates from dev into new process branch\ngit checkout &lt;new_feature&gt;\ngit rebase dev\n# IMPORTANT: It is your responsibility to resolve conflicts with stable code\n\n# Switch to dev and merge new process\ngit checkout dev\ngit merge &lt;new_feature&gt;\ngit branch -d &lt;new_feature&gt; # delete new feature"
  },
  {
    "objectID": "prototyping.html#prototyping",
    "href": "prototyping.html#prototyping",
    "title": "How to use this template",
    "section": "",
    "text": "To build up an analysis project, workflow development aims to follow the GitFlow model of coding.\n\nStable main git branch.\nMostly stable dev git branch - develop a line of analysis.\nFeature branch to try stuff.\n\nMake a new branch named &lt;new_feature&gt; in Git.\ngit checkout -b &lt;new_feature&gt;\nAdd a process into the workflow:\n\nNextflow processes are kept as modular as possible, often limiting them to a single tool. The benefits are that one can often use public images for a process, reducing build time. Environments are small, meaning low build time, and low conflict potential. Since projects are often incrementally developed, this reduces the need to rebuild images or environments, saving time.\nTry to use existing containers (public images) when possible. When a container must be created, I use Docker to make a local image, test, and then push it to Github packages to keep it private to the project. The README.md in the workflow/containers directory has more information on creating custom containers.\n\nprocess &lt;uppercase_name&gt; {\n\n  input:\n  path &lt;filename_var&gt;\n\n  // directives\n  &lt;directives&gt;\n  conda \"&lt;channel&gt;::&lt;software&gt;=&lt;version&gt;\"\n  container \"&lt;software&gt;:&lt;version+build&gt;\"\n\n  script:\n  \"\"\"\n  command --opts $var\n  \"\"\"\n\n  output:\n  path \"&lt;filename&gt;\", emit: &lt;file_type&gt;\n  path \"*\"  // Captures everything. Use when you don't know what the output is.\n\n}\nPass the appropriate input channel(s) in the workflow block.\n\nNextflow has a large selection of channel operators (functions that manipulate process input), which can manipulate input into any format you need. It can be difficult to get the desired input combination first time, and you do not want to experiment on your long running data sets to find out what works. To understand what a Nextflow construct does, write toy examples like in Nextflow Patterns. Nextflow also has a (GUI) console mode to test syntax (nextflow console).\n\nworkflow {\n\n  Channel.fromFilePairs( params.reads, checkIfExists: true )\n    .set{ input_ch }\n\n  FASTQC( input_ch )\n  FASTP( input_ch )\n  ASSEMBLE( FASTP.out.trimmed_reads )\n\n}\nAdd the process resources to the configs.\nprocess {\n  withName: 'FASTQC' {\n    cpus = 4\n    time = '1h'\n  }\n  withName: 'FASTP' {\n    cpus = 4\n    time = '2h'\n  }\n}\nDon’t forget to commit your changes to the feature branch as you develop:\ngit add &lt;file&gt;\ngit commit -m \"What I did\"\nMistakes to commits can be changed using:\ngit commit --amend\n\n\n\nHave a folder under analyses for development (YYYY-MM-DD_workflow_dev)\n\nparams.yml points to test data.\nUse YYYY-MM-DD to label folders and provide a natural ordering.\n\nHave another folder under analyses to analyse the real data set (YYYY-MM-DD_fulldata_analysis)\nRun stuff and break it.\n\nActivate conda environment.\nrun_nextflow.sh\nNextflow prints the working directory when a process fails.\n\n\nSee Troubleshooting for tips on debugging issues.\n\n\n\nOnce it works, merge with the existing development branch or main branch. There are two forms of merging with git. merge tacks on the changes to the end of the branch. rebase undoes your changes, applies commits to bring the branch up to date, and then reapplies your changes.\n# Update dev branch with changes from origin\ngit checkout dev\ngit pull --rebase\n\n# merge updates from dev into new process branch\ngit checkout &lt;new_feature&gt;\ngit rebase dev\n# IMPORTANT: It is your responsibility to resolve conflicts with stable code\n\n# Switch to dev and merge new process\ngit checkout dev\ngit merge &lt;new_feature&gt;\ngit branch -d &lt;new_feature&gt; # delete new feature"
  },
  {
    "objectID": "starting_steps.html",
    "href": "starting_steps.html",
    "title": "How to use this template",
    "section": "",
    "text": "Make a private Project repository from my Template repository on Github.\n\nSelect New Repository on Github from the + symbol in the top right corner.\nSelect mahesh-panchal/NBIS_project_template in Repository template.\nChange owner to NBISweden.\nProvide a repository name following SMS-&lt;id&gt;-&lt;year&gt;-&lt;short_description&gt;.\nEnsure repository is private, then click Create repository.\nAdd a link to the Redmine Project in the URL box.\n\nClone it into the NAISS Compute project.\ncd /proj/naiss20XX-YY-ZZ\ngit clone git@github.com:NBISweden/SMS-&lt;id&gt;-&lt;year&gt;-&lt;short_description&gt;.git NBIS_support_&lt;id&gt;\nClone the NAISS compute project locally.\ncd ~/Documents/Projects\ngit clone &lt;user&gt;@rackham.uppmax.uu.se:/proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;\nUpdate README in the repository.\n\nThe template provides an organised folder structure, and skeleton files to quickly start prototyping. A Makefile is present to run common commands.\nAnalyses are primarily run on Uppmax. Github is used as a backup, and local work is pushed directly to Uppmax saving a step. This means however the Uppmax active branch (usually main) must be different from my local branch ( usually a feature branch ) when I git push.\nOnce the repositories are cloned, I update the README with the project info, tasks to be performed, and Uppmax directories."
  },
  {
    "objectID": "starting_steps.html#starting-a-support-project.",
    "href": "starting_steps.html#starting-a-support-project.",
    "title": "How to use this template",
    "section": "",
    "text": "Make a private Project repository from my Template repository on Github.\n\nSelect New Repository on Github from the + symbol in the top right corner.\nSelect mahesh-panchal/NBIS_project_template in Repository template.\nChange owner to NBISweden.\nProvide a repository name following SMS-&lt;id&gt;-&lt;year&gt;-&lt;short_description&gt;.\nEnsure repository is private, then click Create repository.\nAdd a link to the Redmine Project in the URL box.\n\nClone it into the NAISS Compute project.\ncd /proj/naiss20XX-YY-ZZ\ngit clone git@github.com:NBISweden/SMS-&lt;id&gt;-&lt;year&gt;-&lt;short_description&gt;.git NBIS_support_&lt;id&gt;\nClone the NAISS compute project locally.\ncd ~/Documents/Projects\ngit clone &lt;user&gt;@rackham.uppmax.uu.se:/proj/naiss20XX-YY-ZZ/NBIS_support_&lt;id&gt;\nUpdate README in the repository.\n\nThe template provides an organised folder structure, and skeleton files to quickly start prototyping. A Makefile is present to run common commands.\nAnalyses are primarily run on Uppmax. Github is used as a backup, and local work is pushed directly to Uppmax saving a step. This means however the Uppmax active branch (usually main) must be different from my local branch ( usually a feature branch ) when I git push.\nOnce the repositories are cloned, I update the README with the project info, tasks to be performed, and Uppmax directories."
  },
  {
    "objectID": "troubleshooting.html",
    "href": "troubleshooting.html",
    "title": "How to use this template",
    "section": "",
    "text": "If a Nextflow process fails to run, Nextflow reports the Nextflow working directory it failed in. Change directory to that folder:\ncd /path/to/nextflow/workdir/&lt;xx&gt;/&lt;hashstring&gt;\nIn that folder there are several hidden files starting with .. The .command.sh contains the process script. You can modify this and execute it, but it runs in your current environment ( i.e., outside the container). To run the modified .command.sh inside the container, you should run .command.run, either directly on a worker node ( e.g. bash .command.run) or submit it to the cluster (e.g. sbatch .command.run). One can debug the process script in this way, incorporate changes to the Nextflow workflow, and continue.\n.command.begin      # Script to execute before process script\n.command.err        # Error stream log\n.command.log        # Combined stream log\n.command.out        # Output stream log\n.command.run        # Run script - runs .command.sh in the correct environment.\n.command.sh         # Process script.\n.exitcode\nBe wary of long running commands, and use toy data to make something work. For example:\nblastx ...                                      # Long run time\nawk '&lt;complex script&gt;' &lt;blast_output&gt;           # Quick\nComment out blastx and run awk with toy data. Alternatively comment out the awk, let it finish successfully, and use the cached output.\nOnce you’re done, use nextflow clean -f -before &lt;job_id&gt; to clean up redundant working directories. Use nextflow log to get &lt;job_id&gt;."
  },
  {
    "objectID": "troubleshooting.html#troubleshooting-a-nextflow-process",
    "href": "troubleshooting.html#troubleshooting-a-nextflow-process",
    "title": "How to use this template",
    "section": "",
    "text": "If a Nextflow process fails to run, Nextflow reports the Nextflow working directory it failed in. Change directory to that folder:\ncd /path/to/nextflow/workdir/&lt;xx&gt;/&lt;hashstring&gt;\nIn that folder there are several hidden files starting with .. The .command.sh contains the process script. You can modify this and execute it, but it runs in your current environment ( i.e., outside the container). To run the modified .command.sh inside the container, you should run .command.run, either directly on a worker node ( e.g. bash .command.run) or submit it to the cluster (e.g. sbatch .command.run). One can debug the process script in this way, incorporate changes to the Nextflow workflow, and continue.\n.command.begin      # Script to execute before process script\n.command.err        # Error stream log\n.command.log        # Combined stream log\n.command.out        # Output stream log\n.command.run        # Run script - runs .command.sh in the correct environment.\n.command.sh         # Process script.\n.exitcode\nBe wary of long running commands, and use toy data to make something work. For example:\nblastx ...                                      # Long run time\nawk '&lt;complex script&gt;' &lt;blast_output&gt;           # Quick\nComment out blastx and run awk with toy data. Alternatively comment out the awk, let it finish successfully, and use the cached output.\nOnce you’re done, use nextflow clean -f -before &lt;job_id&gt; to clean up redundant working directories. Use nextflow log to get &lt;job_id&gt;."
  }
]